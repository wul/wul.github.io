---
layout: post
title: "Unicode 以及如何使用 Unicode 编程"
date: 2015-04-07 11:07:36 +0800
comments: true
categories: [Python, Unicode]
---
我们讨论 Unicode和使用编程中使用 Unicode，需要对 Unicode 的一些概念予以澄清，从而知道知道它是什么，它用来解决什么问题。对于很多的程序员来讲，在编程生涯中或多或少听过或者少量用过 Unicode，但是可能对于说清楚它是什么，它如何工作的却没有信心。那么本着回答这些问题的原则，我们来目的性的了解 Unicode 是什么，如何在编程语言中使用（Python）它。

Unicode的历史起源
======
Unicode 的起源源自计算机发展过程中，对于解决多国文字的表示，存储这一问题。

计算机发展初期，并没有这个问题，原因在于，当时的体系结果只处理英文字符即可。这个时候使用的字符集称作 ASCII 码。它将33个控制字符，95个可打印字符的每一个赋予一个7比特的数字。比如，这里面包括了常用的控制键 TAB, \r, \n，数字0-9， 字母A-Za-z等。 这128个字符，用了一个字节的低7位即可表示。对于那个时期的计算机而言足够了，既没有如现在 Mac OSX，Windows 等一样富有表现力的图形界面，也不需要字符界面上做各种花样输出。

当计算机被使用的越来越广泛，越来越多的国家参与到计算机的使用与开发时候，一个新问题马上就浮现出来，那就是对于本国语言的表示示与存储的支持。之前提到的 ASCII 编码集合规定了英文字符用于交互时候的表示，它是通过一个7bit 的数字来表示。那么对于其他的西欧字符，比如字符"ä"就无法映射到0-127这个数值范围内。于是有了扩展 ASCII 的 ISO-8859-1(Lantin-1)字符集。Latin-1字符集用到了其他的一些西欧字符，未能能够对其编码，需要把一个字节的8个比特都用到，于是除过 ASCII 定义好的0-127这个数值范围对应的字符外，扩展了128-255的映射，里面包含了诸如ä之类的额外的西欧字符。如下图所示：摘自 [Wikipedia](http://en.wikipedia.org/wiki/ISO/IEC_8859-1)

![LATIN-1](/images/latin-1.png)

到了这里还不够，即使 LATIN-1这种编码仍然没法包含地球上所有国家所使用的字符，也无法包含常用的诸如汉语这样的字符。因此还的定义更多的字符集。

使用中文来举例子，一些常用的简体汉字最初被收纳在 GB2312这个字符集中。GB2312也定义了这些常用汉字的编码，即其对应的数值，这个数值包含着2个字节组成的数字中，被称作区位码。单单从2个字节可以存放的空间来看，能够定义65536个字符，但是实际上对于 GB2312来讲，它只是用到了其中的一部分。按照相关规范的定义，只是用到了其中的一部分，用来编码简体汉字字符。对于包含更多汉的字字符集，有 GBK，GB18030标准。

选用不同的编码标准，即使对于同一个字符，可能也有不同的编码。比如字符"ä"在cp857（Turkish） 编码中值是132，但在 Latin-1中是228, 在UTF8中则是0xc3a4。代码页(code page)是字符编码的别名，是特定语言字符集的一张表。这张表描述了可支持的字符的集合及其编码。 由于历史原因，Microsoft，IBM，SAP 等都定义了自己的代码页，同一种编码比如“utf-8"在 Windows中代码页是1208，微软是65001，SAP 是4110，显得很混乱。不过这都是历史原因了。目前我们要关注的是 Unicode，以及 Unicode 在内存中和以及传输、存储中的表示。

因此围绕各国语言定义了很多字符集，同时定义了这些字符集如何编码（转换成数字的值）。但是仍然不够，如果选取某种字符集及编码方式，一般只能做到同时支持拉丁字符和选取的字符集（比如简体汉字），无法同时支持诸如中文、韩语、阿拉伯文。因此，出现了由非盈利组织*统一码联盟*负值推广的 Unicode 字符编码方案，旨在赋予每个字符（任何语言）对应一唯一数值的方式来替代现有的字符编码方案。

Unicode 将不同语言中的字符统一编码。其赋予任何语言，任何字符一个唯一的一个值，称作 code point，这个值得范围是 U+0000到 U+10FFFF, 理论上映射1,114,112个字符。因此从空间上来看是够了。 

字符集，Unicode, Coding, Encoding等概念辨析
======

前面说了 Unicode 及其以前关于字符编码的一些基本知识。我们基本可以得到一个概念，这么多年来我们实际上面临了一个文字由人传递给机器的一个问题。那就是，如何让计算机表示人类的字符，并且消除发展过程中形成的“局域”化的标准，转向“统一”标准。

注意，Unicode 只是定义了一个更庞大的字符集，并且把其中的每个字符与一个数字对应起来。但是并没有定义如何存储这些数字，比如汉字的“汉”，对应的 code point 值是 u+6c49, 这个值两个字节，Unicode 并没有说它存储成文件是否应该存贮成"6c49"这种16进制串或者存贮成一个短整数 6c49，如果考虑到 Little Endian 或者 Big Endian 还要考虑顺序。因此，如果考虑到存储，这里还要进行再次的编码。

让我们对编码（Encoding）这个概念进行一下澄清。之前将到字符集以及如何将字符“编码”成数字，是建立一个概念上的字符与数字之间的意义对应关系。这里并不牵扯到这个数字如何表示，在计算机中用几个字节存储，字节顺序等等。它只一个概念上的字符与数字的对应。这里的编码（Encoding）是抽象层次的编码，准确说来是 “编号”（Coding）；而如果考虑这些数字的最终存储形式等实现细节，则是实现层次的编码,比如 UTF-8, UTF-16, UTF-32.  这里举的例子针对 Unicode。对于简单地ASCII 码，它既是指ASCII 字符的编号，同时也是编码。编码这个概念在 Unicode 相关概念中实在是重要，因此需要理解在提到编码的时候到底指的是哪个层次的编码。

使用图例来区分 编号（Coding） 与 编码（Encoding）之间的区别。他们所在的层次不一样，如下面所示。
![Coding/Encoding](/images/encoding.png)

一个某种语言的字符，首先对应一个某种规则定义的数值。这是用来定义某种字符集和它的编号规则。然后，在存储的时候，这个编号根据不同的编码规则，生成二进制序列串（比如 Unicode 字符‘汉’按照不同编码规则生成的字节流不一样）。实际上在看关于 Unicode 相关文章的时候，编号和编码这两个词是混用的，但是我们自己心里得清楚这里有两个层次： 一个是数字表示形式，一个是数字存储形式。

因为我们之后要重点关注 Unicode 字符串的处理，而UTF-8编码使用的比较多，我们这里对 UTF-8编码规则做一个介绍：
UTF-8具体的编码方式

>*   code point 为0-127的（ASCII）编码成一个字节。
>*   128-255的编码成2个字节。
>*   大于0x7ff 的编码成3-4个字节。

当然了，你也可以在存储Unicode 字符串的时候选择 UTF-16方式。那么别人拿到这个文件，他如何得知以什么方式打开这个文件，或者说如何解码呢？
这里有个 BOM（Byte Order Mark）这个概念，在文件的头部，有一些特殊字符，来表明这个文件存储的时候以什么方式编码的。比如文件头碰到“EF BB BF”我们可以得知这个文件是 UTF-8编码，碰见“FE FF”可以得知这个事以 UTF-16进行编码的。完整的列表可以参考[Wikipedia](http://en.wikipedia.org/wiki/Byte_order_mark Wikipedia)

操作系统的支持
======
常用的操作系统 Windows，Linux，Mac OSX都已经支持 Unicode

编程语言的支持
======
Unicode 字符集赋予任意一个字符一个唯一的值。那么我们平常书写的程序中，使用字符串的时候，这些字符串在内存中如何表示呢？对于早期用 ASCII 码字符串，这个问题很好理解。比如对于 C 语言处理 ASCII 字符串，我们一般是在内存中生成一个字节数组，用来存储字符串对应的 ASCII 值，最后用0来结尾。到了 Unicode 时代，我们知道一个字节肯定不行了，那么对于Unicode 字符串，有的语言比如 Python 是在内存中存放这些字符对应的 code points，对于字符串的一些操作如索引，分割，拼接也就是对这些 code points 做相应的动作。最后这些字符串如果想要传输或者存储，则先用诸如 UTF-8, UTF-16等编码然后再处理。

使用 Python 和 Unicode
======
在 Python，尤其是 Python 3.X 中处理 Unicode 字符串异常的方便。我们通过一些例子来了解关于 Unicode 字符串的一些处理。

普通字符串操作
------
声明Unicode字符串，在 Python3中，所有的字符串都是 Unicode 字符串，所以你只要如下面声明一个字符串，那就已经是一个 Unicode 字符串了

    s = '汉字'

那么它的长度呢

    len(s)                                 => 2

完美，并不会出现奇怪的长度居然是4，6啊之类的怪异数字，2个字符就是2个字符，不管它们是什么语言的字符

那么对于一些字符，或许我们键盘没有输入法，但是我知道它的 code point，如何输入呢？

    s = '\u6c49\u5b57'                  => '汉字'

使用_\uNNNN_或者_\UNNNNNNNN_这种方式可以从 code points 来生成 Unicode 字符串。

显式的对 Unicode 字符串进行编码会生成字节数组，参数 encoding 可以选择任何支持的编码方式，大约有100多种，可以参考Python 库手册所定义的编码方式[Standard Encodings](https://docs.python.org/3/library/codecs.html#standard-encodings Standard Encodings). encode、decode 方法的缺省编码都是 utf8。
  
    s.encode(encoding='utf8')             => b'\xe6\xb1\x89\xe5\xad\x97'

从字节数组解码生成字符串对象

    b = s.encode(encoding='utf8')
    s = b.decode(encoding='utf8')             =>'汉字'

另外一种字符串对象与字节数组转换的方式

    b = bytearray(s, 'utf8')      =>b'\xe6\xb1\x89\xe5\xad\x97'
    s = str(b, 'utf8')                 =>'汉字'
          
注意在这个例子中，encoding 不再是字典参数，encoding 参数也没有缺省值，如果不提供的话，可能出现不适期望的结果， 比如

    b = b'abc'
    s = str(b)                =>"b'abc'"
    
字符串并未还原成'abc'，而是生成字符串"b'abc'"

源文件字符集编码声明
------
在写 Python 程序的时候，源文件本身可能含有多国语言，甚至源文件中一些变量名，函数名本身也支持多国语言（Python 支持），比如声明一个变量如下：

    变量 = 100

以中文名定义了一个变量。  这些由非全 ASCII 码构成的源文件本身在存储到硬盘的时候，也需要进行编码。 因此 Python 解释器需要知道在加载这些源文件进行执行的时候，首先要能够读取构成程序的代码字符串。Python 在文件头的注释里面，可以添加如下定义：

    #!/usr/bin/env python
    # -*- coding: <encoding name> -*-

或者

    #!/usr/bin/env python
    # coding: <encoding name>

来指明。

文件操作
------
文件处理操作的模式有两种，文本模式与二进制模式。对于将 utf8格式的文件来说，可以将其看做是文本文件，也可以将其看做是二进制文件。只是在读取其内容还原成字符串对象的时候步骤不同。

首先考虑文本方式，Python3.x 提供的 open 函数增加了一个 encoding 参数，用户当以文本方式打开文件的时候，将会以此参数指定的编码方式来进行编解码。这个过程对于调用者来讲是透明的，自动的。通过 read(), readlines()之类的结果，读取的结果是字符串对象。通过 write(), writelines()接口写入的时候，Python 会自动进行字符串的编码，想字节数组写入文件中。
    
    s = "这是 unicode 字符串"
    with open("unicode.txt", 'w', encoding='utf8') as outf:            
        outf.write(s)
    with open("unicode.txt", 'r') as inf:                    #缺省以 utf8方式打开
        s = inf.readline()
    print(s)                                                 => "这是 unicode 字符串"
    

如果以二进制方式打开的话，那么写入和读取文件都是字节数组。这些字节数组在写入的时候需要调用者调用 encode 方法显式地生成，或者读取的时候使用 decode 方法显式地解码。

    s = "这是 unicode 字符串"
    with open("unicode.txt", 'wb') as outf:
        outf.write(s.encoding())                        #手工编码
    with open("unicode.txt", 'rb') as inf:
        b = inf.read()
    s = b.decode(encoding='utf8')                       #手工解码
    print(s)                                             => "这是 unicode 字符串"
    


到此，基本上应该明白了啥是 Unicode，基本原理是什么以及如何在 Python 中使用。其他的一些语言如 Java, C++没有经验，不知道是否如此方便的处理 Unicode，但是网上搜到的关于 C++关于处理 Unicode 的一些例子，在解决 Unicode 字符串的 Indexing、slicing 以及可移植性上似乎很是麻烦，我不禁陷入了深深的沉思，这种面向内存编程的语言，真是让程序员过多的考虑构建合适的辅助工具而不能专注于要解决的业务问题。面对日新月异的互联网时代，这些语言何去何从呢？

<b>![装B 状-pic from CSDN](http://img.blog.csdn.net/20141031165003296) </b>
<b>[装B 状-pic 引用自 CSDN](http://img.blog.csdn.net/20141031165003296)</b>